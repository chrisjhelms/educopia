#!/bin/tcsh 
#echo $MACACHE_USER 
#echo $MACACHE_PWD 

cat TODAY_DATE > TODAY_STATUS
./PREPWITHDATES >> TODAY_STATUS  
cat STATUS_OF_TODAY 

echo -n "start crawl; enter when crawl done " 
set t = $< 


wget --user $MACACHE_USER --password $MACACHE_PWD -O -  \
	"http://rbdadmin.lib.auburn.edu:8081/DaemonStatus?table=ArchivalUnitTable&key=edu%7Ccmu%7Carchivalware%26auid%7E1%26base_url%7Ehttp%253A%252F%252Fdata%252Emetaarchive%252Eorg%252Fpublic%252Fedu%252Ecmu%252Farchivalware%26collection%7E51&output=text" | \
	fgrep "NodeHasContent=yes" |  \
	sed 's/sort=[0-9]*,NodeChildCount=,//' | \
	sed 's/NodeTreeSize=,NodeHasContent=yes,//' | \
	sed 's/http.*mockup.//' > urlinfo 

wget --user $MACACHE_USER --password $MACACHE_PWD -O - \
	"http://rbdadmin.lib.auburn.edu:8081/ListObjects?type=urls&auid=edu%7Ccmu%7Carchivalware%26auid%7E1%26base_url%7Ehttp%253A%252F%252Fdata%252Emetaarchive%252Eorg%252Fpublic%252Fedu%252Ecmu%252Farchivalware%26collection%7E51" | \
	sed 's/^.*mockup.//' > urllist

echo "give me the GETS" > $$ 
vi $$ 
cat $$ | sed 's/^.*mockup./GET /' > GETS 
rm $$

echo -n " crawl id > " 
set id = $<
echo $id 
mkdir $id 
cd $id
foreach state ('fetched' 'parsed' 'not-modified' 'excluded') 
   echo "get crawlStatus $id $state"  
   /bin/rm $state.csv
   wget -O $state.csv "http://rbdadmin.lib.auburn.edu:8081/DaemonStatus?table=crawl_urls&key=$id.$state&output=csv" --user $MACACHE_USER --password $MACACHE_PWD
end 
cat *  > ../LOCKSS.crawlinfo 


